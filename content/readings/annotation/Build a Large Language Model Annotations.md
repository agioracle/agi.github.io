---
annotation-target: build-a-large-language-model-from-scratch.pdf
annotation-target-type: pdf
---





>%%
>```annotation-json
>{"created":"2025-02-20T01:37:31.594Z","updated":"2025-02-20T01:37:31.594Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":55689,"end":56008},{"type":"TextQuoteSelector","exact":"LLMs have transformed the field of natural language processing, which previ-ously mostly relied on explicit rule-based systems and simpler statistical meth-ods.  The  advent  of  LLMs  introduced  new  deep  learning-driven  approachesthat led to advancements in understanding, generating, and translating humanlanguage","prefix":" this exciting journey!Summary ","suffix":". Modern LLMs are trained in tw"}]}]}
>```
>%%
>*%%PREFIX%%this exciting journey!Summary%%HIGHLIGHT%% ==LLMs have transformed the field of natural language processing, which previ-ously mostly relied on explicit rule-based systems and simpler statistical meth-ods.  The  advent  of  LLMs  introduced  new  deep  learning-driven  approachesthat led to advancements in understanding, generating, and translating humanlanguage== %%POSTFIX%%. Modern LLMs are trained in tw*
>%%LINK%%[[#^tt2tjxgwv7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tt2tjxgwv7


>%%
>```annotation-json
>{"created":"2025-02-20T01:56:57.440Z","updated":"2025-02-20T01:56:57.440Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":56011,"end":56326},{"type":"TextQuoteSelector","exact":"Modern LLMs are trained in two main steps: – First,  they  are  pretrained  on  a  large  corpus  of  unlabeled  text  by  using  theprediction of the next word in a sentence as a label.– Then,  they  are  fine-tuned  on  a  smaller,  labeled  target  dataset  to  followinstructions or perform classification tasks","prefix":"and translating humanlanguage. ","suffix":". LLMs  are  based  on  the  tr"}]}]}
>```
>%%
>*%%PREFIX%%and translating humanlanguage.%%HIGHLIGHT%% ==Modern LLMs are trained in two main steps: – First,  they  are  pretrained  on  a  large  corpus  of  unlabeled  text  by  using  theprediction of the next word in a sentence as a label.– Then,  they  are  fine-tuned  on  a  smaller,  labeled  target  dataset  to  followinstructions or perform classification tasks== %%POSTFIX%%. LLMs  are  based  on  the  tr*
>%%LINK%%[[#^fy6brg9t4iw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^fy6brg9t4iw


>%%
>```annotation-json
>{"created":"2025-02-20T01:57:06.372Z","updated":"2025-02-20T01:57:06.372Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":56329,"end":56578},{"type":"TextQuoteSelector","exact":"LLMs  are  based  on  the  transformer  architecture.  The  key  idea  of  the  trans-former  architecture  is  an  attention  mechanism  that  gives  the  LLM  selectiveaccess to the whole input sequence when generating the output one word ata time","prefix":" perform classification tasks. ","suffix":".  The  original  transformer  "}]}]}
>```
>%%
>*%%PREFIX%%perform classification tasks.%%HIGHLIGHT%% ==LLMs  are  based  on  the  transformer  architecture.  The  key  idea  of  the  trans-former  architecture  is  an  attention  mechanism  that  gives  the  LLM  selectiveaccess to the whole input sequence when generating the output one word ata time== %%POSTFIX%%.  The  original  transformer*
>%%LINK%%[[#^3ojbg57qx2y|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3ojbg57qx2y


>%%
>```annotation-json
>{"created":"2025-02-20T01:57:18.832Z","updated":"2025-02-20T01:57:18.832Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":56582,"end":56703},{"type":"TextQuoteSelector","exact":"The  original  transformer  architecture  consists  of  an  encoder  for  parsing  textand a decoder for generating text.","prefix":"the output one word ata time.  ","suffix":"  LLMs  for  generating  text  "}]}]}
>```
>%%
>*%%PREFIX%%the output one word ata time. %%HIGHLIGHT%% ==The  original  transformer  architecture  consists  of  an  encoder  for  parsing  textand a decoder for generating text.== %%POSTFIX%% LLMs  for  generating  text*
>%%LINK%%[[#^i164oqruwal|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^i164oqruwal


>%%
>```annotation-json
>{"created":"2025-02-20T01:57:28.101Z","updated":"2025-02-20T01:57:28.101Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":56706,"end":56855},{"type":"TextQuoteSelector","exact":"LLMs  for  generating  text  and  following  instructions,  such  as  GPT-3  andChatGPT, only implement decoder modules, simplifying the architecture","prefix":" decoder for generating text.  ","suffix":". Large  datasets  consisting  "}]}]}
>```
>%%
>*%%PREFIX%%decoder for generating text. %%HIGHLIGHT%% ==LLMs  for  generating  text  and  following  instructions,  such  as  GPT-3  andChatGPT, only implement decoder modules, simplifying the architecture== %%POSTFIX%%. Large  datasets  consisting*
>%%LINK%%[[#^kmdme0tfcg|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kmdme0tfcg


>%%
>```annotation-json
>{"created":"2025-02-20T01:57:47.785Z","updated":"2025-02-20T01:57:47.785Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":56951,"end":57165},{"type":"TextQuoteSelector","exact":"While  the  general  pretraining  task  for  GPT-like  models  is  to  predict  the  nextword in a sentence, these LLMs exhibit emergent properties, such as capabili-ties to classify, translate, or summarize texts.","prefix":"sential  for  pretrainingLLMs. ","suffix":"16 CHAPTER 1 Understanding large"}]}]}
>```
>%%
>*%%PREFIX%%sential  for  pretrainingLLMs.%%HIGHLIGHT%% ==While  the  general  pretraining  task  for  GPT-like  models  is  to  predict  the  nextword in a sentence, these LLMs exhibit emergent properties, such as capabili-ties to classify, translate, or summarize texts.== %%POSTFIX%%16 CHAPTER 1 Understanding large*
>%%LINK%%[[#^nutdpa1m1u|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nutdpa1m1u


>%%
>```annotation-json
>{"created":"2025-02-20T01:58:36.006Z","updated":"2025-02-20T01:58:36.006Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":57215,"end":57336},{"type":"TextQuoteSelector","exact":"Once an LLM is pretrained, the resulting foundation model can be fine-tunedmore efficiently for various downstream tasks.","prefix":"standing large language models ","suffix":"  LLMs fine-tuned on custom dat"}]}]}
>```
>%%
>*%%PREFIX%%standing large language models%%HIGHLIGHT%% ==Once an LLM is pretrained, the resulting foundation model can be fine-tunedmore efficiently for various downstream tasks.== %%POSTFIX%% LLMs fine-tuned on custom dat*
>%%LINK%%[[#^mfj5ir9en4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mfj5ir9en4


>%%
>```annotation-json
>{"created":"2025-02-20T01:58:43.930Z","updated":"2025-02-20T01:58:43.930Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":57339,"end":57418},{"type":"TextQuoteSelector","exact":"LLMs fine-tuned on custom datasets can outperform general LLMs on specifictasks","prefix":"for various downstream tasks.  ","suffix":".17Working with text dataSo  far"}]}]}
>```
>%%
>*%%PREFIX%%for various downstream tasks. %%HIGHLIGHT%% ==LLMs fine-tuned on custom datasets can outperform general LLMs on specifictasks== %%POSTFIX%%.17Working with text dataSo  far*
>%%LINK%%[[#^sdvgav3wmbe|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sdvgav3wmbe


>%%
>```annotation-json
>{"created":"2025-02-20T07:49:46.865Z","text":"GPT 为什么不使用其他特殊token呢？","updated":"2025-02-20T07:49:46.865Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":86198,"end":86317},{"type":"TextQuoteSelector","exact":"The tokenizer used for GPT models does not need any of these tokens; it only uses an<|endoftext|> token for simplicity.","prefix":"f the longest text in the batch.","suffix":" <|endoftext|> is analogous to t"}]}]}
>```
>%%
>*%%PREFIX%%f the longest text in the batch.%%HIGHLIGHT%% ==The tokenizer used for GPT models does not need any of these tokens; it only uses an<|endoftext|> token for simplicity.== %%POSTFIX%%<|endoftext|> is analogous to t*
>%%LINK%%[[#^0z3yl1ca0so|show annotation]]
>%%COMMENT%%
>GPT 为什么不使用其他特殊token呢？
>%%TAGS%%
>
^0z3yl1ca0so


>%%
>```annotation-json
>{"created":"2025-02-20T07:51:46.735Z","text":"BPE: byte pair encoding","updated":"2025-02-20T07:51:46.735Z","document":{"title":"Build a Large Language Model (From Scratch)","link":[{"href":"urn:x-pdf:be381473cc3fa226dfca76b2da693c2e"},{"href":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf"}],"documentFingerprint":"be381473cc3fa226dfca76b2da693c2e"},"uri":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","target":[{"source":"vault:/Obsidian-Notes/assets/pdfs/build-a-large-language-model-from-scratch.pdf","selector":[{"type":"TextPositionSelector","start":86667,"end":86849},{"type":"TextQuoteSelector","exact":"GPT models also doesn’t use an <|unk|> tokenfor  out-of-vocabulary  words.  Instead,  GPT  models  use  a  byte  pair  encoding  tokenizer,which breaks words down into subword units,","prefix":"oreover, the tokenizer used for ","suffix":" which we will discuss next.332."}]}]}
>```
>%%
>*%%PREFIX%%oreover, the tokenizer used for%%HIGHLIGHT%% ==GPT models also doesn’t use an <|unk|> tokenfor  out-of-vocabulary  words.  Instead,  GPT  models  use  a  byte  pair  encoding  tokenizer,which breaks words down into subword units,== %%POSTFIX%%which we will discuss next.332.*
>%%LINK%%[[#^c91cuf3339|show annotation]]
>%%COMMENT%%
>BPE: byte pair encoding
>%%TAGS%%
>
^c91cuf3339
